{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Lab 7: Implement skip gram model to predict words within a certain range before and after the current word.**\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "BZ1IWZFqcWw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import string\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "BLWGtMP6zW1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "slYpOqsDWiEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnyBkE3yWjjN",
        "outputId": "18d5e396-7d2e-4f5d-9024-d47c6cc7332a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "#Compute softmax values for each sets of scores in x.\n",
        "\te_x = np.exp(x - np.max(x))\n",
        "\treturn e_x / e_x.sum()\n",
        "\n",
        "class word2vec(object):\n",
        "\tdef __init__(self):\n",
        "\n",
        "#Initialize the word2vec model\n",
        "\t\tself.N = 10\n",
        "\t\tself.X_train = []\n",
        "\t\tself.y_train = []\n",
        "\t\tself.window_size = 2\n",
        "\t\tself.alpha = 0.001\n",
        "\t\tself.words = []\n",
        "\t\tself.word_index = {}\n",
        "\n",
        "#Initialize the model parameters\n",
        "\tdef initialize(self,V,data):\n",
        "\t\tself.V = V\n",
        "\t\tself.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
        "\t\tself.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
        "\n",
        "\t\tself.words = data\n",
        "\t\tfor i in range(len(data)):\n",
        "\t\t\tself.word_index[data[i]] = i\n",
        "\n",
        "#Compute the forward pass of the model\n",
        "\tdef feed_forward(self,X):\n",
        "\t\tself.h = np.dot(self.W.T,X).reshape(self.N,1)\n",
        "\t\tself.u = np.dot(self.W1.T,self.h)\n",
        "\t\t#print(self.u)\n",
        "\t\tself.y = softmax(self.u)\n",
        "\t\treturn self.y\n",
        "\n",
        "#Compute the backward pass of the model\n",
        "\tdef backpropagate(self,x,t):\n",
        "\t\te = self.y - np.asarray(t).reshape(self.V,1)\n",
        "\t\t# e.shape is V x 1\n",
        "\t\tdLdW1 = np.dot(self.h,e.T)\n",
        "\t\tX = np.array(x).reshape(self.V,1)\n",
        "\t\tdLdW = np.dot(X, np.dot(self.W1,e).T)\n",
        "\t\tself.W1 = self.W1 - self.alpha*dLdW1\n",
        "\t\tself.W = self.W - self.alpha*dLdW\n",
        "\n",
        "#Train the word2vec model for `epochs` epochs\n",
        "\tdef train(self,epochs):\n",
        "\t\tfor x in range(1,epochs):\n",
        "\t\t\tself.loss = 0\n",
        "\t\t\tfor j in range(len(self.X_train)):\n",
        "\t\t\t\tself.feed_forward(self.X_train[j])\n",
        "\t\t\t\tself.backpropagate(self.X_train[j],self.y_train[j])\n",
        "\t\t\t\tC = 0\n",
        "\t\t\t\tfor m in range(self.V):\n",
        "\t\t\t\t\tif(self.y_train[j][m]):\n",
        "\t\t\t\t\t\tself.loss += -1*self.u[m][0]\n",
        "\t\t\t\t\t\tC += 1\n",
        "\t\t\t\tself.loss += C*np.log(np.sum(np.exp(self.u)))\n",
        "\t\t\tprint(\"epoch \",x, \" loss = \",self.loss)\n",
        "\t\t\tself.alpha *= 1/( (1+self.alpha*x) )\n",
        "\n",
        "\n",
        "#Predicts the top `number_of_predictions` words for the input word.\n",
        "\tdef predict(self,word,number_of_predictions):\n",
        "\t\tif word in self.words:\n",
        "\t\t\tindex = self.word_index[word]\n",
        "\t\t\tX = [0 for i in range(self.V)]\n",
        "\t\t\tX[index] = 1\n",
        "\t\t\tprediction = self.feed_forward(X)\n",
        "\t\t\toutput = {}\n",
        "\t\t\tfor i in range(self.V):\n",
        "\t\t\t\toutput[prediction[i][0]] = i\n",
        "\n",
        "\t\t\ttop_context_words = []\n",
        "\t\t\tfor k in sorted(output,reverse=True):\n",
        "\t\t\t\ttop_context_words.append(self.words[output[k]])\n",
        "\t\t\t\tif(len(top_context_words)>=number_of_predictions):\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\treturn top_context_words\n",
        "\t\telse:\n",
        "\t\t\tprint(\"Word not found in dictionary\")\n"
      ],
      "metadata": {
        "id": "XQyGdqwJzWxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess the corpus\n",
        "def preprocessing(corpus):\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttraining_data = []\n",
        "\tsentences = corpus.split(\".\")\n",
        "\tfor i in range(len(sentences)):\n",
        "\t\tsentences[i] = sentences[i].strip()\n",
        "\t\tsentence = sentences[i].split()\n",
        "\t\tx = [word.strip(string.punctuation) for word in sentence\n",
        "\t\t\t\t\t\t\t\t\tif word not in stop_words]\n",
        "\t\tx = [word.lower() for word in x]\n",
        "\t\ttraining_data.append(x)\n",
        "\treturn training_data\n",
        "\n",
        "#Prepare the data for training\n",
        "def prepare_data_for_training(sentences,w2v):\n",
        "\tdata = {}\n",
        "\tfor sentence in sentences:\n",
        "\t\tfor word in sentence:\n",
        "\t\t\tif word not in data:\n",
        "\t\t\t\tdata[word] = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tdata[word] += 1\n",
        "\tV = len(data)\n",
        "\tdata = sorted(list(data.keys()))\n",
        "\tvocab = {}\n",
        "\tfor i in range(len(data)):\n",
        "\t\tvocab[data[i]] = i\n",
        "\n",
        "#for i in range(len(words)):\n",
        "\tfor sentence in sentences:\n",
        "\t\tfor i in range(len(sentence)):\n",
        "\t\t\tcenter_word = [0 for x in range(V)]\n",
        "\t\t\tcenter_word[vocab[sentence[i]]] = 1\n",
        "\t\t\tcontext = [0 for x in range(V)]\n",
        "\n",
        "\t\t\tfor j in range(i-w2v.window_size,i+w2v.window_size):\n",
        "\t\t\t\tif i!=j and j>=0 and j<len(sentence):\n",
        "\t\t\t\t\tcontext[vocab[sentence[j]]] += 1\n",
        "\t\t\tw2v.X_train.append(center_word)\n",
        "\t\t\tw2v.y_train.append(context)\n",
        "\tw2v.initialize(V,data)\n",
        "\n",
        "\treturn w2v.X_train,w2v.y_train\n"
      ],
      "metadata": {
        "id": "uwklGjtZzXfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train and evaluate the word2vec model\n",
        "corpus = \"\"\n",
        "corpus += \"The cat sat on the mat.\"\n",
        "epochs = 500\n",
        "\n",
        "training_data = preprocessing(corpus)\n",
        "w2v = word2vec()\n",
        "\n",
        "prepare_data_for_training(training_data,w2v)\n",
        "w2v.train(epochs)\n",
        "\n",
        "print(w2v.predict(\"cat\",3))\n",
        "\n",
        "loss = []\n",
        "for epoch in range(epochs):\n",
        "    loss.append(w2v.loss)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BWhY-9qzZsU",
        "outputId": "9f1f0d23-8d4f-4d69-f11a-e6a91652a297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1  loss =  13.211488313661025\n",
            "epoch  2  loss =  13.19345187855253\n",
            "epoch  3  loss =  13.1754968004144\n",
            "epoch  4  loss =  13.157640395378488\n",
            "epoch  5  loss =  13.139899524043804\n",
            "epoch  6  loss =  13.122290497636813\n",
            "epoch  7  loss =  13.104828991263327\n",
            "epoch  8  loss =  13.08752996535459\n",
            "epoch  9  loss =  13.07040759620966\n",
            "epoch  10  loss =  13.053475216319018\n",
            "epoch  11  loss =  13.036745264928573\n",
            "epoch  12  loss =  13.020229249077259\n",
            "epoch  13  loss =  13.0039377151225\n",
            "epoch  14  loss =  12.98788023056311\n",
            "epoch  15  loss =  12.972065375784272\n",
            "epoch  16  loss =  12.956500745188114\n",
            "epoch  17  loss =  12.9411929570397\n",
            "epoch  18  loss =  12.92614767125283\n",
            "epoch  19  loss =  12.911369614263805\n",
            "epoch  20  loss =  12.896862610093137\n",
            "epoch  21  loss =  12.882629616673043\n",
            "epoch  22  loss =  12.868672766520616\n",
            "epoch  23  loss =  12.854993410858935\n",
            "epoch  24  loss =  12.841592166328176\n",
            "epoch  25  loss =  12.828468963482466\n",
            "epoch  26  loss =  12.815623096332295\n",
            "epoch  27  loss =  12.803053272263536\n",
            "epoch  28  loss =  12.790757661739345\n",
            "epoch  29  loss =  12.778733947268512\n",
            "epoch  30  loss =  12.766979371199973\n",
            "epoch  31  loss =  12.755490781977183\n",
            "epoch  32  loss =  12.7442646785562\n",
            "epoch  33  loss =  12.733297252756433\n",
            "epoch  34  loss =  12.722584429372485\n",
            "epoch  35  loss =  12.712121903929212\n",
            "epoch  36  loss =  12.701905178009131\n",
            "epoch  37  loss =  12.69192959212269\n",
            "epoch  38  loss =  12.68219035612675\n",
            "epoch  39  loss =  12.67268257722629\n",
            "epoch  40  loss =  12.663401285618308\n",
            "epoch  41  loss =  12.65434145785633\n",
            "epoch  42  loss =  12.64549803802893\n",
            "epoch  43  loss =  12.636865956856708\n",
            "epoch  44  loss =  12.628440148820086\n",
            "epoch  45  loss =  12.620215567435078\n",
            "epoch  46  loss =  12.61218719879671\n",
            "epoch  47  loss =  12.604350073510073\n",
            "epoch  48  loss =  12.596699277127822\n",
            "epoch  49  loss =  12.589229959210188\n",
            "epoch  50  loss =  12.581937341119938\n",
            "epoch  51  loss =  12.57481672266024\n",
            "epoch  52  loss =  12.56786348765824\n",
            "epoch  53  loss =  12.561073108591733\n",
            "epoch  54  loss =  12.554441150350582\n",
            "epoch  55  loss =  12.547963273218622\n",
            "epoch  56  loss =  12.541635235156054\n",
            "epoch  57  loss =  12.535452893456446\n",
            "epoch  58  loss =  12.529412205846947\n",
            "epoch  59  loss =  12.523509231094685\n",
            "epoch  60  loss =  12.517740129177367\n",
            "epoch  61  loss =  12.512101161070895\n",
            "epoch  62  loss =  12.506588688202333\n",
            "epoch  63  loss =  12.501199171612013\n",
            "epoch  64  loss =  12.495929170864496\n",
            "epoch  65  loss =  12.490775342744413\n",
            "epoch  66  loss =  12.485734439769365\n",
            "epoch  67  loss =  12.480803308549142\n",
            "epoch  68  loss =  12.475978888017181\n",
            "epoch  69  loss =  12.471258207557648\n",
            "epoch  70  loss =  12.46663838504886\n",
            "epoch  71  loss =  12.462116624841498\n",
            "epoch  72  loss =  12.457690215687958\n",
            "epoch  73  loss =  12.453356528637247\n",
            "epoch  74  loss =  12.449113014908143\n",
            "epoch  75  loss =  12.444957203751715\n",
            "epoch  76  loss =  12.440886700312916\n",
            "epoch  77  loss =  12.436899183499701\n",
            "epoch  78  loss =  12.43299240386694\n",
            "epoch  79  loss =  12.429164181521399\n",
            "epoch  80  loss =  12.425412404053098\n",
            "epoch  81  loss =  12.421735024497663\n",
            "epoch  82  loss =  12.418130059333322\n",
            "epoch  83  loss =  12.414595586515834\n",
            "epoch  84  loss =  12.411129743553776\n",
            "epoch  85  loss =  12.407730725626351\n",
            "epoch  86  loss =  12.40439678374526\n",
            "epoch  87  loss =  12.40112622296185\n",
            "epoch  88  loss =  12.397917400620395\n",
            "epoch  89  loss =  12.394768724658078\n",
            "epoch  90  loss =  12.391678651951917\n",
            "epoch  91  loss =  12.38864568671272\n",
            "epoch  92  loss =  12.385668378925924\n",
            "epoch  93  loss =  12.382745322838975\n",
            "epoch  94  loss =  12.379875155494764\n",
            "epoch  95  loss =  12.377056555310606\n",
            "epoch  96  loss =  12.374288240701992\n",
            "epoch  97  loss =  12.371568968750303\n",
            "epoch  98  loss =  12.368897533913756\n",
            "epoch  99  loss =  12.3662727667805\n",
            "epoch  100  loss =  12.36369353286306\n",
            "epoch  101  loss =  12.361158731432958\n",
            "epoch  102  loss =  12.35866729439467\n",
            "epoch  103  loss =  12.356218185197688\n",
            "epoch  104  loss =  12.353810397785809\n",
            "epoch  105  loss =  12.351442955582414\n",
            "epoch  106  loss =  12.349114910510814\n",
            "epoch  107  loss =  12.346825342048511\n",
            "epoch  108  loss =  12.344573356314413\n",
            "epoch  109  loss =  12.342358085187817\n",
            "epoch  110  loss =  12.340178685458321\n",
            "epoch  111  loss =  12.33803433800551\n",
            "epoch  112  loss =  12.335924247007465\n",
            "epoch  113  loss =  12.3338476391772\n",
            "epoch  114  loss =  12.331803763025952\n",
            "epoch  115  loss =  12.329791888152482\n",
            "epoch  116  loss =  12.327811304557486\n",
            "epoch  117  loss =  12.325861321982195\n",
            "epoch  118  loss =  12.323941269270344\n",
            "epoch  119  loss =  12.32205049375265\n",
            "epoch  120  loss =  12.320188360653042\n",
            "epoch  121  loss =  12.31835425251581\n",
            "epoch  122  loss =  12.316547568652949\n",
            "epoch  123  loss =  12.314767724610963\n",
            "epoch  124  loss =  12.313014151656388\n",
            "epoch  125  loss =  12.311286296279437\n",
            "epoch  126  loss =  12.309583619714985\n",
            "epoch  127  loss =  12.307905597480348\n",
            "epoch  128  loss =  12.306251718929207\n",
            "epoch  129  loss =  12.304621486821079\n",
            "epoch  130  loss =  12.303014416905787\n",
            "epoch  131  loss =  12.301430037522325\n",
            "epoch  132  loss =  12.299867889211637\n",
            "epoch  133  loss =  12.298327524342788\n",
            "epoch  134  loss =  12.296808506751992\n",
            "epoch  135  loss =  12.295310411394066\n",
            "epoch  136  loss =  12.293832824005856\n",
            "epoch  137  loss =  12.292375340781122\n",
            "epoch  138  loss =  12.29093756805657\n",
            "epoch  139  loss =  12.289519122008512\n",
            "epoch  140  loss =  12.288119628359823\n",
            "epoch  141  loss =  12.286738722096821\n",
            "epoch  142  loss =  12.285376047195673\n",
            "epoch  143  loss =  12.284031256357972\n",
            "epoch  144  loss =  12.282704010755207\n",
            "epoch  145  loss =  12.281393979781729\n",
            "epoch  146  loss =  12.280100840815962\n",
            "epoch  147  loss =  12.278824278989491\n",
            "epoch  148  loss =  12.277563986963836\n",
            "epoch  149  loss =  12.276319664714528\n",
            "epoch  150  loss =  12.275091019322296\n",
            "epoch  151  loss =  12.273877764771086\n",
            "epoch  152  loss =  12.272679621752642\n",
            "epoch  153  loss =  12.271496317477459\n",
            "epoch  154  loss =  12.270327585491835\n",
            "epoch  155  loss =  12.269173165500826\n",
            "epoch  156  loss =  12.268032803196911\n",
            "epoch  157  loss =  12.266906250094099\n",
            "epoch  158  loss =  12.265793263367367\n",
            "epoch  159  loss =  12.264693605697175\n",
            "epoch  160  loss =  12.263607045118919\n",
            "epoch  161  loss =  12.262533354877135\n",
            "epoch  162  loss =  12.261472313284253\n",
            "epoch  163  loss =  12.260423703583822\n",
            "epoch  164  loss =  12.259387313817971\n",
            "epoch  165  loss =  12.258362936698974\n",
            "epoch  166  loss =  12.257350369484842\n",
            "epoch  167  loss =  12.25634941385869\n",
            "epoch  168  loss =  12.255359875811859\n",
            "epoch  169  loss =  12.25438156553059\n",
            "epoch  170  loss =  12.25341429728616\n",
            "epoch  171  loss =  12.252457889328348\n",
            "epoch  172  loss =  12.251512163782134\n",
            "epoch  173  loss =  12.250576946547511\n",
            "epoch  174  loss =  12.249652067202291\n",
            "epoch  175  loss =  12.248737358907833\n",
            "epoch  176  loss =  12.247832658317577\n",
            "epoch  177  loss =  12.246937805488281\n",
            "epoch  178  loss =  12.246052643793895\n",
            "epoch  179  loss =  12.245177019841963\n",
            "epoch  180  loss =  12.244310783392487\n",
            "epoch  181  loss =  12.243453787279156\n",
            "epoch  182  loss =  12.242605887332848\n",
            "epoch  183  loss =  12.2417669423074\n",
            "epoch  184  loss =  12.240936813807469\n",
            "epoch  185  loss =  12.240115366218493\n",
            "epoch  186  loss =  12.239302466638666\n",
            "epoch  187  loss =  12.23849798481284\n",
            "epoch  188  loss =  12.237701793068315\n",
            "epoch  189  loss =  12.236913766252442\n",
            "epoch  190  loss =  12.236133781671999\n",
            "epoch  191  loss =  12.235361719034247\n",
            "epoch  192  loss =  12.23459746038968\n",
            "epoch  193  loss =  12.233840890076307\n",
            "epoch  194  loss =  12.233091894665558\n",
            "epoch  195  loss =  12.232350362909617\n",
            "epoch  196  loss =  12.23161618569027\n",
            "epoch  197  loss =  12.230889255969098\n",
            "epoch  198  loss =  12.230169468739078\n",
            "epoch  199  loss =  12.229456720977488\n",
            "epoch  200  loss =  12.228750911600093\n",
            "epoch  201  loss =  12.22805194141655\n",
            "epoch  202  loss =  12.227359713087058\n",
            "epoch  203  loss =  12.22667413108013\n",
            "epoch  204  loss =  12.225995101631515\n",
            "epoch  205  loss =  12.225322532704203\n",
            "epoch  206  loss =  12.22465633394952\n",
            "epoch  207  loss =  12.223996416669209\n",
            "epoch  208  loss =  12.223342693778537\n",
            "epoch  209  loss =  12.222695079770373\n",
            "epoch  210  loss =  12.222053490680185\n",
            "epoch  211  loss =  12.221417844051969\n",
            "epoch  212  loss =  12.220788058905049\n",
            "epoch  213  loss =  12.22016405570174\n",
            "epoch  214  loss =  12.21954575631585\n",
            "epoch  215  loss =  12.218933084001968\n",
            "epoch  216  loss =  12.218325963365572\n",
            "epoch  217  loss =  12.217724320333868\n",
            "epoch  218  loss =  12.217128082127383\n",
            "epoch  219  loss =  12.216537177232272\n",
            "epoch  220  loss =  12.215951535373328\n",
            "epoch  221  loss =  12.215371087487656\n",
            "epoch  222  loss =  12.214795765699026\n",
            "epoch  223  loss =  12.214225503292841\n",
            "epoch  224  loss =  12.213660234691751\n",
            "epoch  225  loss =  12.213099895431856\n",
            "epoch  226  loss =  12.21254442213948\n",
            "epoch  227  loss =  12.211993752508551\n",
            "epoch  228  loss =  12.211447825278514\n",
            "epoch  229  loss =  12.210906580212772\n",
            "epoch  230  loss =  12.210369958077674\n",
            "epoch  231  loss =  12.209837900621991\n",
            "epoch  232  loss =  12.209310350556905\n",
            "epoch  233  loss =  12.208787251536462\n",
            "epoch  234  loss =  12.2082685481385\n",
            "epoch  235  loss =  12.207754185846037\n",
            "epoch  236  loss =  12.207244111029098\n",
            "epoch  237  loss =  12.206738270926953\n",
            "epoch  238  loss =  12.20623661363083\n",
            "epoch  239  loss =  12.20573908806695\n",
            "epoch  240  loss =  12.205245643980046\n",
            "epoch  241  loss =  12.2047562319172\n",
            "epoch  242  loss =  12.204270803212097\n",
            "epoch  243  loss =  12.20378930996963\n",
            "epoch  244  loss =  12.203311705050858\n",
            "epoch  245  loss =  12.202837942058304\n",
            "epoch  246  loss =  12.202367975321627\n",
            "epoch  247  loss =  12.201901759883562\n",
            "epoch  248  loss =  12.20143925148623\n",
            "epoch  249  loss =  12.200980406557747\n",
            "epoch  250  loss =  12.200525182199115\n",
            "epoch  251  loss =  12.200073536171441\n",
            "epoch  252  loss =  12.19962542688341\n",
            "epoch  253  loss =  12.199180813379073\n",
            "epoch  254  loss =  12.198739655325864\n",
            "epoch  255  loss =  12.19830191300295\n",
            "epoch  256  loss =  12.197867547289764\n",
            "epoch  257  loss =  12.197436519654831\n",
            "epoch  258  loss =  12.197008792144853\n",
            "epoch  259  loss =  12.196584327374003\n",
            "epoch  260  loss =  12.196163088513464\n",
            "epoch  261  loss =  12.19574503928121\n",
            "epoch  262  loss =  12.195330143931978\n",
            "epoch  263  loss =  12.194918367247505\n",
            "epoch  264  loss =  12.194509674526918\n",
            "epoch  265  loss =  12.194104031577375\n",
            "epoch  266  loss =  12.1937014047049\n",
            "epoch  267  loss =  12.19330176070538\n",
            "epoch  268  loss =  12.192905066855806\n",
            "epoch  269  loss =  12.192511290905653\n",
            "epoch  270  loss =  12.192120401068486\n",
            "epoch  271  loss =  12.191732366013689\n",
            "epoch  272  loss =  12.191347154858434\n",
            "epoch  273  loss =  12.19096473715974\n",
            "epoch  274  loss =  12.190585082906798\n",
            "epoch  275  loss =  12.190208162513338\n",
            "epoch  276  loss =  12.18983394681026\n",
            "epoch  277  loss =  12.18946240703835\n",
            "epoch  278  loss =  12.189093514841181\n",
            "epoch  279  loss =  12.188727242258139\n",
            "epoch  280  loss =  12.188363561717603\n",
            "epoch  281  loss =  12.188002446030271\n",
            "epoch  282  loss =  12.187643868382596\n",
            "epoch  283  loss =  12.187287802330388\n",
            "epoch  284  loss =  12.186934221792521\n",
            "epoch  285  loss =  12.18658310104478\n",
            "epoch  286  loss =  12.186234414713823\n",
            "epoch  287  loss =  12.185888137771276\n",
            "epoch  288  loss =  12.185544245527929\n",
            "epoch  289  loss =  12.185202713628062\n",
            "epoch  290  loss =  12.184863518043876\n",
            "epoch  291  loss =  12.18452663507005\n",
            "epoch  292  loss =  12.184192041318369\n",
            "epoch  293  loss =  12.183859713712495\n",
            "epoch  294  loss =  12.18352962948283\n",
            "epoch  295  loss =  12.18320176616147\n",
            "epoch  296  loss =  12.182876101577266\n",
            "epoch  297  loss =  12.18255261385097\n",
            "epoch  298  loss =  12.182231281390498\n",
            "epoch  299  loss =  12.18191208288626\n",
            "epoch  300  loss =  12.181594997306586\n",
            "epoch  301  loss =  12.181280003893265\n",
            "epoch  302  loss =  12.180967082157117\n",
            "epoch  303  loss =  12.180656211873714\n",
            "epoch  304  loss =  12.180347373079115\n",
            "epoch  305  loss =  12.180040546065744\n",
            "epoch  306  loss =  12.1797357113783\n",
            "epoch  307  loss =  12.179432849809771\n",
            "epoch  308  loss =  12.179131942397524\n",
            "epoch  309  loss =  12.178832970419426\n",
            "epoch  310  loss =  12.178535915390107\n",
            "epoch  311  loss =  12.178240759057227\n",
            "epoch  312  loss =  12.177947483397865\n",
            "epoch  313  loss =  12.177656070614923\n",
            "epoch  314  loss =  12.177366503133658\n",
            "epoch  315  loss =  12.177078763598207\n",
            "epoch  316  loss =  12.176792834868252\n",
            "epoch  317  loss =  12.176508700015662\n",
            "epoch  318  loss =  12.176226342321293\n",
            "epoch  319  loss =  12.175945745271743\n",
            "epoch  320  loss =  12.175666892556265\n",
            "epoch  321  loss =  12.175389768063647\n",
            "epoch  322  loss =  12.17511435587922\n",
            "epoch  323  loss =  12.17484064028187\n",
            "epoch  324  loss =  12.17456860574114\n",
            "epoch  325  loss =  12.174298236914344\n",
            "epoch  326  loss =  12.174029518643783\n",
            "epoch  327  loss =  12.173762435953968\n",
            "epoch  328  loss =  12.173496974048906\n",
            "epoch  329  loss =  12.173233118309435\n",
            "epoch  330  loss =  12.172970854290618\n",
            "epoch  331  loss =  12.172710167719162\n",
            "epoch  332  loss =  12.17245104449089\n",
            "epoch  333  loss =  12.172193470668262\n",
            "epoch  334  loss =  12.171937432477943\n",
            "epoch  335  loss =  12.171682916308388\n",
            "epoch  336  loss =  12.17142990870751\n",
            "epoch  337  loss =  12.171178396380352\n",
            "epoch  338  loss =  12.170928366186812\n",
            "epoch  339  loss =  12.170679805139414\n",
            "epoch  340  loss =  12.170432700401111\n",
            "epoch  341  loss =  12.170187039283126\n",
            "epoch  342  loss =  12.169942809242821\n",
            "epoch  343  loss =  12.169699997881628\n",
            "epoch  344  loss =  12.169458592942973\n",
            "epoch  345  loss =  12.169218582310293\n",
            "epoch  346  loss =  12.168979954005025\n",
            "epoch  347  loss =  12.168742696184678\n",
            "epoch  348  loss =  12.168506797140898\n",
            "epoch  349  loss =  12.168272245297608\n",
            "epoch  350  loss =  12.168039029209128\n",
            "epoch  351  loss =  12.167807137558388\n",
            "epoch  352  loss =  12.167576559155094\n",
            "epoch  353  loss =  12.167347282934012\n",
            "epoch  354  loss =  12.1671192979532\n",
            "epoch  355  loss =  12.166892593392319\n",
            "epoch  356  loss =  12.166667158550968\n",
            "epoch  357  loss =  12.166442982847014\n",
            "epoch  358  loss =  12.166220055814986\n",
            "epoch  359  loss =  12.165998367104468\n",
            "epoch  360  loss =  12.165777906478553\n",
            "epoch  361  loss =  12.16555866381227\n",
            "epoch  362  loss =  12.165340629091085\n",
            "epoch  363  loss =  12.165123792409403\n",
            "epoch  364  loss =  12.164908143969104\n",
            "epoch  365  loss =  12.164693674078078\n",
            "epoch  366  loss =  12.16448037314882\n",
            "epoch  367  loss =  12.16426823169703\n",
            "epoch  368  loss =  12.164057240340219\n",
            "epoch  369  loss =  12.163847389796366\n",
            "epoch  370  loss =  12.163638670882595\n",
            "epoch  371  loss =  12.16343107451382\n",
            "epoch  372  loss =  12.1632245917015\n",
            "epoch  373  loss =  12.163019213552346\n",
            "epoch  374  loss =  12.162814931267057\n",
            "epoch  375  loss =  12.162611736139109\n",
            "epoch  376  loss =  12.162409619553534\n",
            "epoch  377  loss =  12.162208572985712\n",
            "epoch  378  loss =  12.162008588000205\n",
            "epoch  379  loss =  12.161809656249627\n",
            "epoch  380  loss =  12.161611769473442\n",
            "epoch  381  loss =  12.161414919496892\n",
            "epoch  382  loss =  12.161219098229886\n",
            "epoch  383  loss =  12.161024297665895\n",
            "epoch  384  loss =  12.160830509880878\n",
            "epoch  385  loss =  12.160637727032261\n",
            "epoch  386  loss =  12.160445941357855\n",
            "epoch  387  loss =  12.160255145174858\n",
            "epoch  388  loss =  12.160065330878842\n",
            "epoch  389  loss =  12.159876490942768\n",
            "epoch  390  loss =  12.159688617915988\n",
            "epoch  391  loss =  12.1595017044233\n",
            "epoch  392  loss =  12.159315743164006\n",
            "epoch  393  loss =  12.159130726910943\n",
            "epoch  394  loss =  12.158946648509612\n",
            "epoch  395  loss =  12.158763500877232\n",
            "epoch  396  loss =  12.158581277001858\n",
            "epoch  397  loss =  12.158399969941522\n",
            "epoch  398  loss =  12.158219572823329\n",
            "epoch  399  loss =  12.158040078842635\n",
            "epoch  400  loss =  12.157861481262197\n",
            "epoch  401  loss =  12.157683773411327\n",
            "epoch  402  loss =  12.157506948685102\n",
            "epoch  403  loss =  12.157331000543543\n",
            "epoch  404  loss =  12.157155922510823\n",
            "epoch  405  loss =  12.156981708174488\n",
            "epoch  406  loss =  12.156808351184692\n",
            "epoch  407  loss =  12.156635845253428\n",
            "epoch  408  loss =  12.156464184153783\n",
            "epoch  409  loss =  12.156293361719207\n",
            "epoch  410  loss =  12.15612337184278\n",
            "epoch  411  loss =  12.155954208476494\n",
            "epoch  412  loss =  12.155785865630559\n",
            "epoch  413  loss =  12.15561833737269\n",
            "epoch  414  loss =  12.155451617827431\n",
            "epoch  415  loss =  12.155285701175483\n",
            "epoch  416  loss =  12.155120581653026\n",
            "epoch  417  loss =  12.154956253551074\n",
            "epoch  418  loss =  12.154792711214814\n",
            "epoch  419  loss =  12.154629949042977\n",
            "epoch  420  loss =  12.154467961487207\n",
            "epoch  421  loss =  12.154306743051436\n",
            "epoch  422  loss =  12.154146288291269\n",
            "epoch  423  loss =  12.153986591813402\n",
            "epoch  424  loss =  12.153827648274993\n",
            "epoch  425  loss =  12.153669452383097\n",
            "epoch  426  loss =  12.153511998894087\n",
            "epoch  427  loss =  12.153355282613077\n",
            "epoch  428  loss =  12.153199298393352\n",
            "epoch  429  loss =  12.153044041135832\n",
            "epoch  430  loss =  12.15288950578851\n",
            "epoch  431  loss =  12.152735687345913\n",
            "epoch  432  loss =  12.15258258084857\n",
            "epoch  433  loss =  12.152430181382496\n",
            "epoch  434  loss =  12.15227848407866\n",
            "epoch  435  loss =  12.152127484112484\n",
            "epoch  436  loss =  12.151977176703332\n",
            "epoch  437  loss =  12.151827557114013\n",
            "epoch  438  loss =  12.151678620650294\n",
            "epoch  439  loss =  12.15153036266042\n",
            "epoch  440  loss =  12.151382778534625\n",
            "epoch  441  loss =  12.151235863704667\n",
            "epoch  442  loss =  12.151089613643359\n",
            "epoch  443  loss =  12.150944023864128\n",
            "epoch  444  loss =  12.150799089920527\n",
            "epoch  445  loss =  12.15065480740583\n",
            "epoch  446  loss =  12.150511171952559\n",
            "epoch  447  loss =  12.150368179232068\n",
            "epoch  448  loss =  12.1502258249541\n",
            "epoch  449  loss =  12.150084104866382\n",
            "epoch  450  loss =  12.149943014754182\n",
            "epoch  451  loss =  12.149802550439924\n",
            "epoch  452  loss =  12.149662707782758\n",
            "epoch  453  loss =  12.149523482678173\n",
            "epoch  454  loss =  12.149384871057602\n",
            "epoch  455  loss =  12.149246868888012\n",
            "epoch  456  loss =  12.149109472171542\n",
            "epoch  457  loss =  12.1489726769451\n",
            "epoch  458  loss =  12.148836479280009\n",
            "epoch  459  loss =  12.148700875281612\n",
            "epoch  460  loss =  12.148565861088928\n",
            "epoch  461  loss =  12.148431432874274\n",
            "epoch  462  loss =  12.148297586842915\n",
            "epoch  463  loss =  12.148164319232711\n",
            "epoch  464  loss =  12.14803162631376\n",
            "epoch  465  loss =  12.147899504388077\n",
            "epoch  466  loss =  12.147767949789223\n",
            "epoch  467  loss =  12.147636958882002\n",
            "epoch  468  loss =  12.147506528062102\n",
            "epoch  469  loss =  12.14737665375579\n",
            "epoch  470  loss =  12.14724733241958\n",
            "epoch  471  loss =  12.147118560539907\n",
            "epoch  472  loss =  12.146990334632836\n",
            "epoch  473  loss =  12.146862651243726\n",
            "epoch  474  loss =  12.146735506946937\n",
            "epoch  475  loss =  12.146608898345526\n",
            "epoch  476  loss =  12.146482822070944\n",
            "epoch  477  loss =  12.146357274782744\n",
            "epoch  478  loss =  12.146232253168291\n",
            "epoch  479  loss =  12.146107753942468\n",
            "epoch  480  loss =  12.145983773847398\n",
            "epoch  481  loss =  12.145860309652164\n",
            "epoch  482  loss =  12.14573735815252\n",
            "epoch  483  loss =  12.145614916170626\n",
            "epoch  484  loss =  12.145492980554788\n",
            "epoch  485  loss =  12.145371548179162\n",
            "epoch  486  loss =  12.145250615943525\n",
            "epoch  487  loss =  12.145130180772979\n",
            "epoch  488  loss =  12.145010239617719\n",
            "epoch  489  loss =  12.144890789452768\n",
            "epoch  490  loss =  12.144771827277733\n",
            "epoch  491  loss =  12.144653350116544\n",
            "epoch  492  loss =  12.144535355017217\n",
            "epoch  493  loss =  12.144417839051613\n",
            "epoch  494  loss =  12.14430079931519\n",
            "epoch  495  loss =  12.14418423292677\n",
            "epoch  496  loss =  12.144068137028324\n",
            "epoch  497  loss =  12.143952508784706\n",
            "epoch  498  loss =  12.143837345383446\n",
            "epoch  499  loss =  12.143722644034526\n",
            "['sat', 'cat', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss vs. Epochs\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SWZvRPPyzkfA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "593b1a2d-bcd8-453d-de69-ffc8fa574dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kq4i0DvTWqWu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
