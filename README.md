# Deep_Learning_Practicals

Welcome to this repository, your hub for exploring various deep learning concepts through practical experiments! Here, you'll find implementations and documentation for a wide range of algorithms, from fundamental building blocks to complex architectures.

# What's inside?

This repository is a living testament to your deep learning journey, containing practical implementations of diverse algorithms:

# Fundamentals:
  - DL MPneuron Perceptron: Explore the basic unit of neural networks,
   the perceptron, with code for both single and multi-layer implementations.
   
  - Sigmoid Activation: Understand and implement the sigmoid activation function, 
   a key component in building non-linear relationships.
   
  - Multiclass Neural Network: Dive into classifying multiple categories with a multiclass neural network, 
   tackling the challenges of one-hot encoding and softmax activation.


# Optimization Techniques:
  - Linear Regression with SGD: Implement linear regression using Stochastic Gradient Descent (SGD),
    understanding the iterative learning process.
    
  - Mini-Batch Gradient Descent: Discover how mini-batch SGD improves efficiency by optimizing over smaller batches of data.

  - Optimization Techniques: Experiment with various optimization algorithms like momentum, Adam, and RMSprop, analyzing their impact on convergence and performance.



# Natural Language Processing:

  - Skip-gram: Explore the word embedding technique called skip-gram, capturing semantic relationships between words.

# Computer Vision:

  - LeNet: Implement the pioneering LeNet architecture for image classification, a foundational model in Deep Learning for vision tasks.

  - ResNet: Build and understand ResNet, a deep convolutional neural network architecture that tackles the vanishing gradient problem.

  - VGG16: Train and analyze the VGG16 architecture, a deep convolutional network famous for its superior performance on image classification tasks.

# Advanced Architectures:

  - Attention Neural Network: Implement an attention-based neural network, understanding its ability to focus on relevant parts of the input sequence.




















